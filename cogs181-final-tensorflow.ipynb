{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-v0Wxs_Kq0ce"
   },
   "source": [
    "# Assignment Description:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Assignment Page: https://sites.google.com/site/ucsdcogs181fall2017/classroom-news\n",
    "\n",
    "Due date: 11:59Pm, 12/16\n",
    "\n",
    "Late Policy: 5% of the total points will be deducted for the first day past due and every 10% of the total points will be deducted for every extra day afterwards.\n",
    "\n",
    "Report format: Write a report with >1,500 words including main sections: a) abstract, b) introduction, c) method, d) experiment, e) conclusion, and f) references. You can follow the paper format as e.g leading machine learning journals such as Journal of Machine Learning Research (http://www.jmlr.org/) or IEEE Trans. on Pattern Analysis and Machine Intelligence (http://www.computer.org/web/tpami), or leading conferences like NIPS (https://papers.nips.cc/) and ICML (http://icml.cc/2016/?page_id=151). \n",
    "\n",
    "Bonus points: If you feel that your work deserves bonus points due to reasons such as: a) novel ideas and applications, b) large efforts in your own data collection/preparation, c) state-of-the-art results on your applications, or d) new algorithms or neural network architecture, please create a \"Bonus Points\" section to specifically describe why you deserve bonus points. In general, we evaluation your justifications based on the review guidelines based on e.g. CVPR/NIPS/ICCV/ICLR.\n",
    "\n",
    "\n",
    "Note that  requirement for the word count (>1,500)  only applies to a single-student project. For team-based projects, each team only needs to write one final report but the role of each team member needs to be clearly defined ans specified. The final project report is also supposed to be much longer thatn 1,500 words, depending upon how many (maximum 3) members there are in your team.\n",
    "\n",
    "Word count:\n",
    "One-person team: >1,500\n",
    "Two-persons team: > 2,200\n",
    "Three-persons team: > 2,900\n",
    "\n",
    "See below a link about writing a scientific paper: http://abacus.bates.edu/~ganderso/biology/resources/writing/HTWtoc.html The format of your references can be of any kind that is adopted in the above journals or conferences.\n",
    "\n",
    "Grading: The merit and grading of your project can be judged from aspects described below that are common when reviewing a paper: \n",
    "1. Interestingness of the problem you are studying. (10 points).\n",
    "\n",
    "2. How challenging and large is the dataset you are studying? (10 points)\n",
    "\n",
    "3. Any aspects that are new in terms of algorithm development, uniqueness of the data, or new applications? (20 points)\n",
    "\n",
    "4. Is your experimental design comprehensive? Have you done thoroughly experiments in tuning hyper parameters? (30 points)\n",
    "\n",
    "Tuning hyper-parameters in your final project will need to be more comprehensive that what was done in HW4.\n",
    "For example, if you are performing CNN classification on the Tiny ImageNet dataset, some options to consider include\n",
    "a. Comparing two different architectures chosen from e.g. LeNet, AlexNet, VGG, GoogleNet or ResNet\n",
    "b. Trying to vary the number of layers.\n",
    "c. Trying to adopt different optimization methods, for example Adam vs. stocahstic gradient descent\n",
    "d. Trying different pooling functions, average pooling, max pooling, stochastic pooling\n",
    "e. Trying to use different activation functions such as ReLu, Sigmoid etc.\n",
    " \n",
    "See e.g. how the significance was justified in the ResNet pape (you don't have to follow this paper though): https://arxiv.org/pdf/1512.03385.pdf\n",
    " \n",
    "5. Is your report written in a professional way with sections including abstract, introduction, method/architecture description, experiments ( data and problem description, hyper-parameters, training process etc.), conclusion, and references? (30 points)\n",
    "\n",
    "6. Bonus points will be assigned to projects that have adopted new methods, worked on novel applications, and/or have done a thorough comparison against the existing methods and possible choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZCWUibBsOnN"
   },
   "source": [
    "### Option (1): Convolutional neural networks Train a convolutional neural networks method on Tiny ImageNet dataset \n",
    "    Link : http://pages.ucsd.edu/~ztu/courses/tiny-imagenet-200.zip)\n",
    "\n",
    "You can choose any deep learning platforms including MatConvNet (http://www.vlfeat.org/matconvnet/), Theano (http://deeplearning.net/software/theano/), Torch (http://torch.ch/), Caffe (http://caffe.berkeleyvision.org/), MxNet (https://github.com/dmlc/mxnet), and TensorFlow.\n",
    "\n",
    "See in the link other possible platforms you can use: http://deeplearning.net/software_links/ You can train a model by building your own network structure or by adopting/following standard networks like AlexNet, GoogLeNet, VGG, etc. You are also welcome to use datasets other than ImageNet, e.g. CIFAR-10, and Kaggle datasets (https://www.kaggle.com/) such as the deep sea image competition: https://www.kaggle.com/c/datasciencebowl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0q097e_b5Uy"
   },
   "source": [
    "### Data source and Related Paper\n",
    "- https://tiny-imagenet.herokuapp.com/\n",
    "- http://cs231n.stanford.edu/reports/2017/pdfs/937.pdf\n",
    "- http://cs231n.stanford.edu/reports/2017/pdfs/940.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aghMOeKbo11"
   },
   "source": [
    "# Setup, Download and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 170,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2555,
     "status": "ok",
     "timestamp": 1513036568083,
     "user": {
      "displayName": "Tony Lim",
      "photoUrl": "//lh3.googleusercontent.com/-DO9616bnct4/AAAAAAAAAAI/AAAAAAAADOc/rjhjTHF-kxU/s50-c-k-no/photo.jpg",
      "userId": "100029816334508670274"
     },
     "user_tz": 480
    },
    "id": "0pSng7KtuSoW",
    "outputId": "d0ba9a0f-5fd5-4339-b2a1-1be674c193f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow-gpu\n",
      "Version: 1.4.0.dev20171012\n",
      "Summary: TensorFlow helps the tensors flow\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: opensource@google.com\n",
      "License: Apache 2.0\n",
      "Location: /opt/conda/lib/python2.7/site-packages\n",
      "Requires: enum34, backports.weakref, wheel, mock, tensorflow-tensorboard, numpy, protobuf, six\n",
      "Name: Keras\n",
      "Version: 2.0.8\n",
      "Summary: Deep Learning for Python\n",
      "Home-page: https://github.com/fchollet/keras\n",
      "Author: Francois Chollet\n",
      "Author-email: francois.chollet@gmail.com\n",
      "License: MIT\n",
      "Location: /opt/conda/lib/python2.7/site-packages\n",
      "Requires: pyyaml, six, scipy, numpy\n"
     ]
    }
   ],
   "source": [
    "# To determine which version you're using:\n",
    "!pip show tensorflow-gpu\n",
    "!pip show keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'/device:GPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "  \n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "g7fRT5eTuVQr"
   },
   "outputs": [],
   "source": [
    "!cd /tmp; wget --quiet http://pages.ucsd.edu/~ztu/courses/tiny-imagenet-200.zip\n",
    "!cd /tmp; unzip -qq tiny-imagenet-200.zip; rm tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_32fTulcA9_"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VkBIPDH_4yPi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.ndimage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "def distort(filename):\n",
    "    \"\"\"Apply image distortions\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        file = tf.read_file(filename)\n",
    "        img = tf.image.decode_jpeg(file, 3)\n",
    "        img = tf.image.adjust_saturation(img, 0.5)\n",
    "        img = tf.image.adjust_hue(img, -0.05)\n",
    "        with tf.Session() as sess:\n",
    "            dist_img = sess.run(img)\n",
    "    \n",
    "    return dist_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGQ2FlzicDuN"
   },
   "source": [
    "### Data Exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "p9nGkMtpsCFs"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "path = \"/tmp/tiny-imagenet-200/\"\n",
    "train_dirs = glob.glob(path + \"train/*\")\n",
    "val_dirs   = glob.glob(path + \"val/*\")\n",
    "test_dirs  = glob.glob(path + \"test/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DfQNHZk3KoZ"
   },
   "source": [
    "#### Loading All Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 527,
     "status": "ok",
     "timestamp": 1513051145558,
     "user": {
      "displayName": "Tony Lim",
      "photoUrl": "//lh3.googleusercontent.com/-DO9616bnct4/AAAAAAAAAAI/AAAAAAAADOc/rjhjTHF-kxU/s50-c-k-no/photo.jpg",
      "userId": "100029816334508670274"
     },
     "user_tz": 480
    },
    "id": "KqGZvL1mvGpx",
    "outputId": "a7f7d4e0-c1a3-4a38-be47-dff3b6eb1148"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is global labels, there are 18K of them, but only 200 is used for training and validation\n",
    "labels = []\n",
    "for line in open(path + 'words.txt'):\n",
    "    [classname, description ] = line.strip().split('\\t')\n",
    "    labels.append( (classname,description) )\n",
    "label_dicts = dict(labels)\n",
    "\n",
    "# There are only 200 used labels in both training and validation\n",
    "used_labels =  [ d[-9:] for d in train_dirs ]\n",
    "len(used_labels)\n",
    "\n",
    "# Hot-Encode Labels\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "bin_encoder = LabelBinarizer()\n",
    "bin_encoder.fit_transform(used_labels).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wjOwALs23NdV"
   },
   "source": [
    "#### Loading Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_paths = []\n",
    "train_labels = []\n",
    "for class_path in train_dirs:\n",
    "    class_name = class_path[-9:]\n",
    "    images = glob.glob(class_path + '/images/*')\n",
    "    for image_path in images:\n",
    "        train_labels.append( class_name )\n",
    "        train_data_paths.append( image_path )\n",
    "        \n",
    "inter = list(zip(train_data_paths, train_labels))\n",
    "random.shuffle(inter)\n",
    "train_data_paths, train_labels = zip(*inter)\n",
    "\n",
    "train_labels = bin_encoder.transform(train_labels) # hot-encode labels from string to number\n",
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(img_path):\n",
    "    img = scipy.ndimage.imread(img_path)\n",
    "    if len(img.shape) != 3: # reshape back to RBG format\n",
    "        img = np.repeat(img[:,:,np.newaxis], 3, axis=2)\n",
    "    return img\n",
    "\n",
    "def get_images(paths):\n",
    "    X = []\n",
    "    for p in paths:\n",
    "        X.append(get_image(p))\n",
    "    return X\n",
    "        \n",
    "def prepare_batch(X,y,batch_size=50):\n",
    "    N = len(y) / batch_size\n",
    "    i = 0\n",
    "    yield X[:batch_size], y[:batch_size]\n",
    "    while i < N:\n",
    "        yield X[batch_size*i:batch_size*(i+1)], y[batch_size*i:batch_size*(i+1)]\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Fw4XvvF525O2"
   },
   "outputs": [],
   "source": [
    "# inter = list(zip(train_images, train_labels))\n",
    "# random.shuffle(inter)\n",
    "# images, labels = zip(*inter)\n",
    "# train_data = DataSet( np.array(train_images), np.array(train_labels), reshape=False)\n",
    "# del train_images, train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i4DBoIC9ED9w"
   },
   "source": [
    "### Loading Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "d9tihkh-B5oP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels = []\n",
    "val_fns = []\n",
    "val_images = []\n",
    "for line in open(path + 'val/val_annotations.txt'):\n",
    "  [fn, classname, _ , _, _, _ ] = line.strip().split('\\t')\n",
    "  \n",
    "  img = scipy.ndimage.imread(path +'val/images/' + fn )\n",
    "  if len(img.shape) != 3: # reshape back to RBG format\n",
    "      img = np.repeat(img[:,:,np.newaxis], 3, axis=2)\n",
    "      \n",
    "  val_labels.append(classname)\n",
    "  val_images.append(img)\n",
    "\n",
    "# hot-encode labels from string to number\n",
    "# val_labels = encoder.transform(val_labels)\n",
    "val_labels = bin_encoder.transform(val_labels)\n",
    "\n",
    "val_data = DataSet( np.array(val_images), np.array(val_labels), reshape=False)\n",
    "del val_labels, val_images\n",
    "val_data.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 64, 64, 3), (2000, 200))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mini-validation set\n",
    "_x_val, _y_val = val_data.next_batch(2000)\n",
    "_x_val.shape, _y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.34509804,  0.29411765,  0.12156863],\n",
       "        [ 0.34117647,  0.28235294,  0.11372549],\n",
       "        [ 0.32941176,  0.2745098 ,  0.09803922],\n",
       "        ..., \n",
       "        [ 0.30980392,  0.25882353,  0.09411765],\n",
       "        [ 0.32941176,  0.27843137,  0.11372549],\n",
       "        [ 0.34117647,  0.28627451,  0.13333333]],\n",
       "\n",
       "       [[ 0.35294118,  0.30196078,  0.12941176],\n",
       "        [ 0.34901961,  0.29019608,  0.12156863],\n",
       "        [ 0.3372549 ,  0.28235294,  0.10588235],\n",
       "        ..., \n",
       "        [ 0.31372549,  0.2627451 ,  0.09803922],\n",
       "        [ 0.33333333,  0.28235294,  0.11764706],\n",
       "        [ 0.34509804,  0.29411765,  0.12941176]],\n",
       "\n",
       "       [[ 0.36078431,  0.30980392,  0.1372549 ],\n",
       "        [ 0.35686275,  0.29803922,  0.12941176],\n",
       "        [ 0.34509804,  0.29019608,  0.11372549],\n",
       "        ..., \n",
       "        [ 0.31764706,  0.26666667,  0.10196078],\n",
       "        [ 0.3372549 ,  0.28627451,  0.12156863],\n",
       "        [ 0.34901961,  0.29803922,  0.13333333]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.27058824,  0.25098039,  0.1254902 ],\n",
       "        [ 0.28627451,  0.26666667,  0.14117647],\n",
       "        [ 0.25882353,  0.24705882,  0.11764706],\n",
       "        ..., \n",
       "        [ 0.01960784,  0.03137255,  0.00392157],\n",
       "        [ 0.02352941,  0.03529412,  0.00784314],\n",
       "        [ 0.02352941,  0.03529412,  0.00784314]],\n",
       "\n",
       "       [[ 0.2627451 ,  0.25098039,  0.12156863],\n",
       "        [ 0.26666667,  0.25490196,  0.1254902 ],\n",
       "        [ 0.23137255,  0.21960784,  0.09019608],\n",
       "        ..., \n",
       "        [ 0.00392157,  0.03529412,  0.        ],\n",
       "        [ 0.00784314,  0.03921569,  0.        ],\n",
       "        [ 0.01176471,  0.04313725,  0.        ]],\n",
       "\n",
       "       [[ 0.2627451 ,  0.25098039,  0.12156863],\n",
       "        [ 0.25098039,  0.23921569,  0.10980392],\n",
       "        [ 0.20392157,  0.19215686,  0.0627451 ],\n",
       "        ..., \n",
       "        [ 0.00392157,  0.03921569,  0.        ],\n",
       "        [ 0.        ,  0.04313725,  0.        ],\n",
       "        [ 0.00392157,  0.04705882,  0.        ]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_image(train_data_paths[0]) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfAEt-8zcFxL"
   },
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_RY9ds0NEocN"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Network Parameters\n",
    "n_classes = 200\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pooling2d(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 64, 64, 3], name='x')\n",
    "Y = tf.placeholder(\"float\", [None, n_classes], name='y_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fM9Fy2IZEVti"
   },
   "outputs": [],
   "source": [
    "class ConvNet():\n",
    "    \n",
    "    def __init__(self, conv_fn, pooling_fn, activation_fn):\n",
    "        self._conv_fn = conv_fn\n",
    "        self._pooling_fn = pooling_fn\n",
    "        self._activation_fn = activation_fn\n",
    "    \n",
    "    def conv_layer(self, x, n_filters, kernel_size):\n",
    "        shape = [kernel_size] * 2\n",
    "        shape = shape + [x.shape[-1].value, n_filters]\n",
    "        \n",
    "        w = weight_variable(shape)\n",
    "        b = bias_variable([n_filters])\n",
    "        return self._activation_fn(self._conv_fn(x, w) + b)\n",
    "    \n",
    "    def pool_layer(self, x, n_strides, kernel_size):\n",
    "        return self._pooling_fn(x)\n",
    "    \n",
    "    def fc_layer(self, x, n_neurons):\n",
    "        mult = lambda x, y: x * y\n",
    "        \n",
    "        shape = x.get_shape().as_list()[1:]\n",
    "        flat_shape = reduce(mult, shape, 1)\n",
    "        \n",
    "        w = weight_variable([flat_shape, n_neurons])\n",
    "        b = bias_variable([n_neurons])\n",
    "        x_flat = tf.reshape(x, [-1, flat_shape])\n",
    "        fc = self._activation_fn(tf.matmul(x_flat, w) + b)\n",
    "        return fc\n",
    "    \n",
    "    def out_layer(self, x, n_classes):\n",
    "        w = weight_variable([x.shape[-1].value, n_classes])\n",
    "        b = bias_variable([n_classes])\n",
    "        return tf.matmul(x, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GoiaIrfs2FRx"
   },
   "source": [
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8FQJfHYt2GEx"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "# Convolutions use a stride of one\n",
    "# Plain old max pooling over 2x2 blocks\n",
    "# Activation function is the ReLu function\n",
    "conv_net = ConvNet(conv2d, max_pooling2d, tf.nn.relu)\n",
    "\n",
    "# MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "# Reshape to match picture format [Height x Width x Channel]\n",
    "x_image = tf.reshape(X, [-1, 64, 64, 3], name='X')\n",
    "\n",
    "# Convolution Layer with 32 filters and a kernel size of 5\n",
    "conv1 = conv_net.conv_layer(x_image, 32, 5)\n",
    "\n",
    "# Convolution Layer with 64 filters and a kernel size of 5\n",
    "conv2 = conv_net.conv_layer(conv1, 64, 5)\n",
    "\n",
    "# Fully connected layer with 1024 neurons\n",
    "fc = conv_net.fc_layer(conv2, 1024)\n",
    "\n",
    "# Applying a dropout before the readout layer to reduce\n",
    "# overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "fc = tf.nn.dropout(fc, keep_prob)\n",
    "\n",
    "# Readout layer\n",
    "y_conv = conv_net.out_layer(fc, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 286,
     "output_extras": [
      {
       "item_id": 5
      },
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 584814,
     "status": "error",
     "timestamp": 1513051779388,
     "user": {
      "displayName": "Tony Lim",
      "photoUrl": "//lh3.googleusercontent.com/-DO9616bnct4/AAAAAAAAAAI/AAAAAAAADOc/rjhjTHF-kxU/s50-c-k-no/photo.jpg",
      "userId": "100029816334508670274"
     },
     "user_tz": 480
    },
    "id": "HB0giFqzGEky",
    "outputId": "7817e3fd-b298-4dc9-f431-664e2140ccfb"
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=Y))\n",
    "optimizer     = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step    = optimizer.minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(Y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "train_loss,train_acc  = [],[]\n",
    "val_acc   , test_acc  = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000, training accuracy 0\n",
      "step 4000, training accuracy 0\n",
      "step 6000, training accuracy 0\n",
      "step 8000, training accuracy 0\n",
      "step 10000, training accuracy 0\n",
      "step 12000, training accuracy 0\n",
      "step 14000, training accuracy 0\n",
      "step 16000, training accuracy 0\n",
      "step 18000, training accuracy 0\n",
      "step 20000, training accuracy 0\n",
      "step 22000, training accuracy 0\n",
      "step 24000, training accuracy 0\n",
      "step 26000, training accuracy 0\n",
      "step 28000, training accuracy 0\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "epoch = 15             # two pass on all training_data\n",
    "batch_size = 25        # in each iteration, train 50, total iteration = 100K / batch_size (in the case of 25 batch size, 4K iteration)\n",
    "\n",
    "# Use interactive session instead\n",
    "# with tf.Session() as sess:\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "i = 0\n",
    "for e in range(epoch):\n",
    "    for img_paths, batch_y in prepare_batch(train_data_paths, train_labels, batch_size=batch_size):\n",
    "        batch_x = get_images(img_paths)\n",
    "        _, loss = sess.run([train_step, cross_entropy], feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.5})\n",
    "        train_accuracy = accuracy.eval(session=sess, feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
    "        train_acc.append(train_accuracy)\n",
    "        \n",
    "#         val_accuracy   = accuracy.eval(session=sess, feed_dict={X: _x_val, Y: _y_val  , keep_prob: 1.0}) # mini-validation sample, because not enough memory\n",
    "#         val_acc  .append(val_accuracy)\n",
    "\n",
    "        train_loss.append(loss)\n",
    "        i += 1\n",
    "        if i % 2000 == 0:\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy 0.025\n",
      "val accuracy 0.025\n",
      "val accuracy 0.032\n",
      "val accuracy 0.031\n",
      "val accuracy 0.022\n",
      "val accuracy 0.033\n",
      "val accuracy 0.021\n",
      "val accuracy 0.025\n",
      "val accuracy 0.018\n",
      "val accuracy 0.027\n"
     ]
    }
   ],
   "source": [
    "print('val accuracy %g' % accuracy.eval(session=sess, feed_dict={ X: val_data.images[:1000], Y: val_data.labels[:1000], keep_prob: 1.0}))\n",
    "for i in range(9):\n",
    "    print('val accuracy %g' % accuracy.eval(session=sess, feed_dict={ X: val_data.images[ i*1000 : (i+1)*1000 ], Y: val_data.labels[ i*1000:(i+1)*1000], keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAFpCAYAAACmgZ0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZIk5cDQJUmSlANDlyRJUg7+f7C8v3a1XqMAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fce78771dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_acc, color='g')\n",
    "# plt.plot(val_acc  , color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "COGS181-Final.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
